{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c34d1886-ba7d-4c5c-9b4f-a1536b14effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import copy\n",
    "import time\n",
    "import scipy.io as io\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from spatial_diffusion_field import SpatialDiffusionField, AgentField\n",
    "\n",
    "\n",
    "ACTION_MAP = {\n",
    "    0:\"left\",1:\"right\",2:\"up\",3:\"down\", \\\n",
    "    4:\"up-left\",5:\"up-right\",6:\"down-left\",7:\"down-right\"\n",
    "}\n",
    "\n",
    "ACTIONS = [\"left\", \"right\", \"up\", \"down\", \\\n",
    "            \"up-left\", \"up-right\", \"down-left\", \"down-right\"]\n",
    "\n",
    "ACTION_OFFSET = [[-1,0],[1,0],[0,-1],[0,1],[-1,-1],[1,-1],[-1,1],[1,1]]\n",
    "\n",
    "class BlankFieldEnv(gym.Env):\n",
    "    def __init__(self, max_num_steps=100):\n",
    "        metadata = {'render.modes': ['human']}\n",
    "        super(BlankFieldEnv, self).__init__()\n",
    "                    \n",
    "        self.max_num_steps = max_num_steps\n",
    "        \n",
    "        # Env Field\n",
    "        self.size = 10\n",
    "        self.field = np.ones((10,10), dtype=np.uint8) * 255\n",
    "        indices = np.argwhere(self.field == 255)\n",
    "        self.centroids = KMeans(n_clusters=5, random_state=0).fit(indices).cluster_centers_[:,::-1]\n",
    "        print(self.centroids)\n",
    "\n",
    "        # Agent Field related params/variables\n",
    "        self.num_steps = 0\n",
    "        self.agent_position = None\n",
    "        self.agent_trajectory = []\n",
    "\n",
    "        # Action Space\n",
    "        self.action_space_map = ACTION_MAP\n",
    "        self.actions = ACTIONS\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "\n",
    "        # Environment Observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.size, self.size, 1), dtype=np.uint8)    \n",
    "\n",
    "    def step(self, action_id):\n",
    "        assert self.action_space.contains(action_id), \"Action is invalid!\"\n",
    "        (next_position, hit_wall, revisit) = self.get_next_position(action_id)\n",
    "\n",
    "        # Update number of steps\n",
    "        self.num_steps += 1\n",
    "\n",
    "        # Get any observations\n",
    "        observations = {\"location\": next_position}\n",
    "\n",
    "        # Update agent variables\n",
    "        self.field[self.agent_position[1],self.agent_position[0]] = 0\n",
    "        self.field[next_position[1],next_position[0]] = 128\n",
    "        self.agent_position = next_position\n",
    "        self.agent_trajectory.append(self.agent_position)\n",
    "        self.update_min_distances(self.agent_position)\n",
    "        \n",
    "        # Check for termination criteria\n",
    "        done = (self.num_steps >= self.max_num_steps)\n",
    "        if done:\n",
    "            reward = self.calculate_done_reward()\n",
    "        else:\n",
    "            reward = -1 if hit_wall else 0\n",
    "        \n",
    "        # Record field values\n",
    "        next_state = np.expand_dims(self.field, axis=-1)\n",
    "        return (next_state, reward, done, observations)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent related params\n",
    "        self.num_steps = 0\n",
    "        self.agent_position = self.choose_random_start_position()\n",
    "        self.field = np.ones((10,10), dtype=np.uint8) * 255\n",
    "        self.field[self.agent_position[1], self.agent_position[0]]=0\n",
    "        self.agent_trajectory = []\n",
    "        self.min_distances = None\n",
    "        self.initial_min_distances = None\n",
    "        self.update_min_distances(self.agent_position)\n",
    "        return np.expand_dims(self.field, axis=-1)\n",
    "    \n",
    "    def update_min_distances(self, pos):\n",
    "        distances = np.linalg.norm(self.centroids-pos, axis=1)\n",
    "        if self.min_distances is None:\n",
    "            self.min_distances = distances\n",
    "            self.initial_min_distances = distances\n",
    "        else:\n",
    "            self.min_distances = np.minimum(self.min_distances, distances)\n",
    "\n",
    "    def choose_random_start_position(self):\n",
    "        possible_starts = [[0,0]]\n",
    "        return random.choice(possible_starts)\n",
    "\n",
    "    def get_next_position(self, action_id):\n",
    "        # Create a deepcopy of current state\n",
    "        next_state = copy.deepcopy(self.agent_position)\n",
    "        next_state = np.add(next_state, ACTION_OFFSET[action_id])\n",
    "\n",
    "        # Check for collisions\n",
    "        hit_wall = False\n",
    "        if ((next_state[0] < 0 or next_state[0] >= self.size) or\n",
    "            (next_state[1] < 0 or next_state[1] >= self.size)):\n",
    "            hit_wall = True\n",
    "        if not hit_wall:\n",
    "            revisit = self.field[next_state[1],next_state[0]]==-1\n",
    "        else:\n",
    "            revisit = False\n",
    "            next_state = self.agent_position\n",
    "        return (next_state, hit_wall, revisit)\n",
    "\n",
    "    def calculate_step_reward(self, next_pos, hit_wall, revisit):\n",
    "        return (-10) if hit_wall else 0 + (-1) if revisit else 0\n",
    "\n",
    "    def calculate_done_reward(self):\n",
    "        # min_distances_improvement = np.sum(np.linalg.norm(self.initial_min_distances-self.min_distances))\n",
    "        # return min_distances_improvement * 10\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c8daa11-efb8-4f3d-bccf-0fcf3db7e2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "    \n",
    "    \n",
    "class PPO_Agent:\n",
    "    def __init__(self, env, name='PPO', path='./output', load_saved_model=False):\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.model_path = os.path.join(path, name)\n",
    "        self.checkpoint_callback = CheckpointCallback(\n",
    "            save_freq=10000,\n",
    "            save_path=f\"./output/checkpoints/{name}/\",\n",
    "            name_prefix=self.name,\n",
    "            save_replay_buffer=True,\n",
    "            save_vecnormalize=True,\n",
    "        )\n",
    "        self.policy_kwargs = dict(\n",
    "            features_extractor_class=CustomCNN,\n",
    "            features_extractor_kwargs=dict(features_dim=128),\n",
    "        )\n",
    "        if load_saved_model and os.path.exists(self.model_path+'.zip'):\n",
    "            self.model = PPO.load(self.model_path, env=env)\n",
    "        else:        \n",
    "            self.model = PPO('CnnPolicy', env, policy_kwargs=self.policy_kwargs, verbose=1)\n",
    "    \n",
    "    def train(self, n_timestep=40_000, eval=True):\n",
    "        self.model.learn(total_timesteps=n_timestep, progress_bar=False, callback=[self.checkpoint_callback])\n",
    "        self.model.save(self.model_path)\n",
    "        if eval:\n",
    "            mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "            print(f'Evaluation [{self.name}] ==> mean_reward: {mean_reward}, std_reward: {std_reward}')        \n",
    "\n",
    "    def predict(self, obs, lstm_state=None, start=False):\n",
    "        action, hidden_state  = self.model.predict(obs, state=lstm_state, episode_start=start)\n",
    "        return action, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71c94af1-7ed5-48f5-a87d-73548aaf39dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/miniconda3/envs/dl/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/thinh/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.0625     4.75      ]\n",
      " [7.22727273 1.77272727]\n",
      " [1.76190476 1.66666667]\n",
      " [7.22727273 7.22727273]\n",
      " [1.57894737 7.42105263]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m BlankFieldEnv()\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO_Agent(env, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/experiments/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_timestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TEST_EPISODES):\n",
      "Cell \u001b[0;32mIn[55], line 59\u001b[0m, in \u001b[0;36mPPO_Agent.train\u001b[0;34m(self, n_timestep, eval)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_timestep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40_000\u001b[39m, \u001b[38;5;28meval\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/policies.py:617\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m    619\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/policies.py:640\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m:param obs: Observation\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m:return: the output of the features extractor(s)\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     pi_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/stable_baselines3/common/policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m :return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[55], line 34\u001b[0m, in \u001b[0;36mCustomCNN.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/container.py:216\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/container.py:207\u001b[0m, in \u001b[0;36mSequential.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39misdigit()]\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keys\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Module]:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# NB: We can't really type check this function as the type of input\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# may change dynamically (as is tested in\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TestScript.test_sequential_intermediary_types).  Cannot annotate\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# with Any as TorchScript expects a more precise type\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_STEPS = 300_000\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# train & evaluate stable-baseline3 model\n",
    "env = BlankFieldEnv()\n",
    "model = PPO_Agent(env, path='./output/experiments/')\n",
    "model.train(n_timestep=N_STEPS)\n",
    "\n",
    "# evaluate\n",
    "for episode_num in range(TEST_EPISODES):\n",
    "    obs = env.reset()\n",
    "    done = True\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, hidden_state = model.predict(obs, start=done)\n",
    "        obs, reward, done, observation = env.step(action.item())\n",
    "        if done:\n",
    "            plt.imshow(env.field)\n",
    "            break\n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bc4e054-8b8e-4428-a280-a40d01d2b6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1abcf6d7f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAADWCAYAAAAZ1mK8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUOElEQVR4nO3dcUyU9+HH8c8BcpQNsChwUEHtnFJrAQXFa13FycrQEN1vfzBnAiXWpYk2deeyybKJ7ZbQpXNlmbTaNI5srdFuqXTrNh3FKGlLh2DvN22qGdZW6jjQuR5ym6fl7veHv1zHT0CpPDx8/b1fyTfxvvf9Hh+eGPn43HMPjnA4HBYAAIAhouwOAAAAMBqUFwAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMvKy8WLF7V27VolJiZq8uTJWrdunfr7+0fcU1RUJIfDMWg8+uijVkUEAAAGclj1u41KS0vV3d2tXbt26erVq6qqqtLChQu1Z8+eYfcUFRVp9uzZevLJJyNz8fHxSkxMtCIiAAAwUIwVL/ree+/pwIEDOnr0qAoKCiRJv/jFL7RixQr99Kc/VUZGxrB74+Pj5XK5rIgFAABuA5aUl9bWVk2ePDlSXCSpuLhYUVFR+stf/qKvfe1rw+596aWX9OKLL8rlcqmsrEw//OEPFR8fP+z6YDCoYDAYeRwKhXTx4kVNmTJFDodjbL4hAABgqXA4rEuXLikjI0NRUSNf1WJJefH5fEpNTR38hWJilJycLJ/PN+y+b37zm5o+fboyMjL017/+Vd/73vd06tQpvfLKK8Puqa2t1RNPPDFm2QEAgH26uro0bdq0EdeMqrxs2bJFP/nJT0Zc8957743mJQf51re+Ffnzfffdp/T0dC1fvlynT5/WF77whSH3VFdXy+PxRB77/X5lZWVpiVYoRpM+cxZgLL3/1EK7IwDX+e//2m13BCCirz+k6Qs+UEJCwg3Xjqq8bN68WQ8//PCIa+6++265XC719vYOmv/kk0908eLFUV3PUlhYKEnq7Owctrw4nU45nc7r5mM0STEOygsmhqi4OLsjANdJTOBuGZh4buaSj1GVl5SUFKWkpNxwndvt1scff6yOjg7l5+dLkg4dOqRQKBQpJDfD6/VKktLT00cTEwAA3MYsqd333HOPvvrVr2r9+vVqa2vTm2++qY0bN+ob3/hG5JNG586dU3Z2ttra2iRJp0+f1o9+9CN1dHTogw8+0O9+9ztVVFTowQcfVE5OjhUxAQCAgSw7Z/jSSy8pOztby5cv14oVK7RkyRI9//zzkeevXr2qU6dO6V//+pckKTY2Vq+//roeeughZWdna/Pmzfr617+u3//+91ZFBAAABrLk00aSlJycPOIN6WbMmKH/vD9eZmamjhw5YlUcAABwm+BqLQAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMoLAAAwCuUFAAAYhfICAACMQnkBAABGobwAAACjUF4AAIBRKC8AAMAolBcAAGAUygsAADAK5QUAABiF8gIAAIxCeQEAAEahvAAAAKNQXgAAgFEoLwAAwCiUFwAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo4xLeamvr9eMGTMUFxenwsJCtbW1jbj+N7/5jbKzsxUXF6f77rtPf/zjH8cjJgAAMIDl5WXfvn3yeDyqqanRsWPHlJubq5KSEvX29g65/q233tKaNWu0bt06vfPOO1q9erVWr16tEydOWB0VAAAYwBEOh8NWfoHCwkItXLhQO3bskCSFQiFlZmbqscce05YtW65bX15erkAgoNdeey0yt3jxYuXl5Wnnzp03/Hp9fX1KSkpSkVYpxjFp7L4R4BZ0PrPY7gjAdU6X3/jfVGC89F0K6c7Z78vv9ysxMXHEtZaeebly5Yo6OjpUXFz86ReMilJxcbFaW1uH3NPa2jpovSSVlJQMuz4YDKqvr2/QAAAAty9Ly8uFCxc0MDCgtLS0QfNpaWny+XxD7vH5fKNaX1tbq6SkpMjIzMwcm/AAAGBCMv7TRtXV1fL7/ZHR1dVldyQAAGChGCtffOrUqYqOjlZPT8+g+Z6eHrlcriH3uFyuUa13Op1yOp1jExgAAEx4lp55iY2NVX5+vpqbmyNzoVBIzc3NcrvdQ+5xu92D1ktSU1PTsOsBAMD/L5aeeZEkj8ejyspKFRQUaNGiRaqrq1MgEFBVVZUkqaKiQnfddZdqa2slSY8//riWLl2q7du3a+XKldq7d6/a29v1/PPPWx0VAAAYwPLyUl5ervPnz2vr1q3y+XzKy8vTgQMHIhflnj17VlFRn54Auv/++7Vnzx794Ac/0Pe//3198YtfVGNjo+bNm2d1VAAAYADL7/My3rjPCyYi7vOCiYj7vGAimTD3eQEAABhrlBcAAGAUygsAADAK5QUAABiF8gIAAIxCeQEAAEahvAAAAKNQXgAAgFEoLwAAwCiUFwAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMoLAAAwCuUFAAAYhfICAACMQnkBAABGobwAAACjUF4AAIBRKC8AAMAolBcAAGAUygsAADAK5QUAABiF8gIAAIxCeQEAAEahvAAAAKOMS3mpr6/XjBkzFBcXp8LCQrW1tQ27tqGhQQ6HY9CIi4sbj5gAAMAAlpeXffv2yePxqKamRseOHVNubq5KSkrU29s77J7ExER1d3dHxocffmh1TAAAYAjLy8vPfvYzrV+/XlVVVZo7d6527typ+Ph47d69e9g9DodDLpcrMtLS0qyOCQAADGFpebly5Yo6OjpUXFz86ReMilJxcbFaW1uH3dff36/p06crMzNTq1at0rvvvjvs2mAwqL6+vkEDAADcviwtLxcuXNDAwMB1Z07S0tLk8/mG3DNnzhzt3r1br776ql588UWFQiHdf//9+uijj4ZcX1tbq6SkpMjIzMwc8+8DAABMHBPu00Zut1sVFRXKy8vT0qVL9corryglJUW7du0acn11dbX8fn9kdHV1jXNiAAAwnmKsfPGpU6cqOjpaPT09g+Z7enrkcrlu6jUmTZqk+fPnq7Ozc8jnnU6nnE7nLWcFAABmsPTMS2xsrPLz89Xc3ByZC4VCam5ultvtvqnXGBgY0PHjx5Wenm5VTAAAYBBLz7xIksfjUWVlpQoKCrRo0SLV1dUpEAioqqpKklRRUaG77rpLtbW1kqQnn3xSixcv1qxZs/Txxx/r6aef1ocffqhHHnnE6qgAAMAAlpeX8vJynT9/Xlu3bpXP51NeXp4OHDgQuYj37Nmzior69ATQP//5T61fv14+n0933nmn8vPz9dZbb2nu3LlWRwUAAAZwhMPhsN0hxlJfX5+SkpJUpFWKcUyyOw4gSep8ZrHdEYDrnC7faXcEIKLvUkh3zn5ffr9fiYmJI66dcJ82AgAAGAnlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMoLAAAwCuUFAAAYhfICAACMQnkBAABGobwAAACjUF4AAIBRKC8AAMAolBcAAGAUygsAADAK5QUAABiF8gIAAIxCeQEAAEahvAAAAKNQXgAAgFEoLwAAwCiUFwAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSwtLy0tLSorK1NGRoYcDocaGxtvuOfw4cNasGCBnE6nZs2apYaGBisjAgAAw1haXgKBgHJzc1VfX39T68+cOaOVK1dq2bJl8nq92rRpkx555BEdPHjQypgAAMAgMVa+eGlpqUpLS296/c6dOzVz5kxt375dknTPPffojTfe0DPPPKOSkhKrYgIAAINMqGteWltbVVxcPGiupKREra2tw+4JBoPq6+sbNAAAwO1rQpUXn8+ntLS0QXNpaWnq6+vTv//97yH31NbWKikpKTIyMzPHIyoAALDJhCovn0V1dbX8fn9kdHV12R0JAABYyNJrXkbL5XKpp6dn0FxPT48SExN1xx13DLnH6XTK6XSORzwAADABTKgzL263W83NzYPmmpqa5Ha7bUoEAAAmGkvLS39/v7xer7xer6RrH4X2er06e/aspGtv+VRUVETWP/roo3r//ff13e9+VydPntSzzz6rl19+Wd/+9retjAkAAAxiaXlpb2/X/PnzNX/+fEmSx+PR/PnztXXrVklSd3d3pMhI0syZM/WHP/xBTU1Nys3N1fbt2/XCCy/wMWkAABBh6TUvRUVFCofDwz4/1N1zi4qK9M4771iYCgAAmGxCXfMCAABwI5QXAABgFMoLAAAwCuUFAAAYhfICAACMQnkBAABGobwAAACjUF4AAIBRKC8AAMAolBcAAGAUygsAADAK5QUAABiF8gIAAIxCeQEAAEahvAAAAKNQXgAAgFEoLwAAwCiUFwAAYBTKCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMoLAAAwCuUFAAAYhfICAACMQnkBAABGsbS8tLS0qKysTBkZGXI4HGpsbBxx/eHDh+VwOK4bPp/PypgAAMAglpaXQCCg3Nxc1dfXj2rfqVOn1N3dHRmpqakWJQQAAKaJsfLFS0tLVVpaOup9qampmjx58tgHAgAAxrO0vHxWeXl5CgaDmjdvnrZt26YHHnhg2LXBYFDBYDDy2O/3S5I+0VUpbHlU4KaELl+2OwJwnb5LIbsjABF9/df+PobDN/HDOzxOJIX3798/4pqTJ0+Gd+7cGW5vbw+/+eab4aqqqnBMTEy4o6Nj2D01NTVhXaspDAaDwWAwDB9dXV037BSO/y0WlnM4HNq/f79Wr149qn1Lly5VVlaWfv3rXw/5/P898xIKhXTx4kVNmTJFDofjViJbqq+vT5mZmerq6lJiYqLdcYzFcRw7HMuxw7EcGxzHsWPCsQyHw7p06ZIyMjIUFTXyJbkT8m2j/7Ro0SK98cYbwz7vdDrldDoHzZl0vUxiYuKE/YtkEo7j2OFYjh2O5djgOI6diX4sk5KSbmrdhL/Pi9frVXp6ut0xAADABGHpmZf+/n51dnZGHp85c0Zer1fJycnKyspSdXW1zp07p1/96leSpLq6Os2cOVP33nuvLl++rBdeeEGHDh3Sn//8ZytjAgAAg1haXtrb27Vs2bLIY4/HI0mqrKxUQ0ODuru7dfbs2cjzV65c0ebNm3Xu3DnFx8crJydHr7/++qDXuF04nU7V1NRc95YXRofjOHY4lmOHYzk2OI5j53Y7luN2wS4AAMBYmPDXvAAAAPwnygsAADAK5QUAABiF8gIAAIxCeQEAAEahvNigvr5eM2bMUFxcnAoLC9XW1mZ3JOO0tLSorKxMGRkZcjgcamxstDuSsWpra7Vw4UIlJCQoNTVVq1ev1qlTp+yOZZznnntOOTk5kTuYut1u/elPf7I71m3hqaeeksPh0KZNm+yOYpxt27bJ4XAMGtnZ2XbHumWUl3G2b98+eTwe1dTU6NixY8rNzVVJSYl6e3vtjmaUQCCg3Nxc1dfX2x3FeEeOHNGGDRv09ttvq6mpSVevXtVDDz2kQCBgdzSjTJs2TU899ZQ6OjrU3t6uL3/5y1q1apXeffddu6MZ7ejRo9q1a5dycnLsjmKse++9V93d3ZEx0q/cMQX3eRlnhYWFWrhwoXbs2CHp2i+SzMzM1GOPPaYtW7bYnM5Mn/WXfmJo58+fV2pqqo4cOaIHH3zQ7jhGS05O1tNPP61169bZHcVI/f39WrBggZ599ln9+Mc/Vl5enurq6uyOZZRt27apsbFRXq/X7ihjijMv4+jKlSvq6OhQcXFxZC4qKkrFxcVqbW21MRnwKb/fL+naD158NgMDA9q7d68CgYDcbrfdcYy1YcMGrVy5ctC/mRi9v/3tb8rIyNDdd9+ttWvXDrqzvakm/G+Vvp1cuHBBAwMDSktLGzSflpamkydP2pQK+FQoFNKmTZv0wAMPaN68eXbHMc7x48fldrt1+fJlff7zn9f+/fs1d+5cu2MZae/evTp27JiOHj1qdxSjFRYWqqGhQXPmzFF3d7eeeOIJfelLX9KJEyeUkJBgd7zPjPICIGLDhg06ceLEbfGeuB3mzJkjr9crv9+v3/72t6qsrNSRI0coMKPU1dWlxx9/XE1NTYqLi7M7jtFKS0sjf87JyVFhYaGmT5+ul19+2ei3Mykv42jq1KmKjo5WT0/PoPmenh65XC6bUgHXbNy4Ua+99ppaWlo0bdo0u+MYKTY2VrNmzZIk5efn6+jRo/r5z3+uXbt22ZzMLB0dHert7dWCBQsicwMDA2ppadGOHTsUDAYVHR1tY0JzTZ48WbNnz1ZnZ6fdUW4J17yMo9jYWOXn56u5uTkyFwqF1NzczPvisE04HNbGjRu1f/9+HTp0SDNnzrQ70m0jFAopGAzaHcM4y5cv1/Hjx+X1eiOjoKBAa9euldfrpbjcgv7+fp0+fVrp6el2R7klnHkZZx6PR5WVlSooKNCiRYtUV1enQCCgqqoqu6MZpb+/f9D/HM6cOSOv16vk5GRlZWXZmMw8GzZs0J49e/Tqq68qISFBPp9PkpSUlKQ77rjD5nTmqK6uVmlpqbKysnTp0iXt2bNHhw8f1sGDB+2OZpyEhITrrrn63Oc+pylTpnAt1ih95zvfUVlZmaZPn66///3vqqmpUXR0tNasWWN3tFtCeRln5eXlOn/+vLZu3Sqfz6e8vDwdOHDguot4MbL29nYtW7Ys8tjj8UiSKisr1dDQYFMqMz333HOSpKKiokHzv/zlL/Xwww+PfyBD9fb2qqKiQt3d3UpKSlJOTo4OHjyor3zlK3ZHw/9jH330kdasWaN//OMfSklJ0ZIlS/T2228rJSXF7mi3hPu8AAAAo3DNCwAAMArlBQAAGIXyAgAAjEJ5AQAARqG8AAAAo1BeAACAUSgvAADAKJQXAABgFMoLAAAwCuUFAAAYhfICAACM8j+PJkJDXch5fgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array([[-1,-1,0,0,1,1],[-1,-1,0,0,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5938c-2139-42b5-938d-5212306aa509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
